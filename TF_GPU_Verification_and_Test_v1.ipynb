{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247c9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Comment out this cell if you want to see warnings about depreciations, upgrades, etc, involving Cuda/Cudnn/Tensorflow\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "# Additional lines to suppress TensorFlow warnings\n",
    "#import tensorflow as tf\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef126be-7baf-4de0-a479-3d7a6949c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python and Driver Version: 3.8.10 (default, Nov 14 2022, 12:59:47) \n",
      "[GCC 9.4.0]\n",
      "TensorFlow Version: 2.13.0\n",
      "1 Physical GPU(s), 1 Logical GPU(s)\n",
      "GPU set as the default device.\n",
      "GPU found?: True\n",
      "Num GPUs Available: 1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11087282862648905092\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7917797376\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13853296519165163734\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print Python, Driver, and TensorFlow versions\n",
    "print(\"Python and Driver Version:\", sys.version)\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Set GPU as the Default Device\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU(s),\", len(logical_gpus), \"Logical GPU(s)\")\n",
    "        print(\"GPU set as the default device.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Check GPU Availability\n",
    "GPU_LIST = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU found?:\", len(GPU_LIST) > 0)\n",
    "print(\"Num GPUs Available:\", len(GPU_LIST))\n",
    "\n",
    "# Display Local Devices\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d8c890-a851-42f2-bb05-a370aa80707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15d659-7a39-4436-81b6-b93325f3a1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc2e1c5-d519-4459-b2f8-a9d1d7a9017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU average execution time: 0.0016772317886352538 seconds\n",
      "TensorFlow CPU average execution time: 0.007544354200363159 seconds\n",
      "Torch GPU average execution time: 0.008020776309967033 seconds\n",
      "Torch CPU average execution time: 0.012086472654342652 seconds\n"
     ]
    }
   ],
   "source": [
    "### Calculates/prints the average execution time for each iteration\n",
    "### for TensorFlow GPU, TensorFlow CPU, Torch GPU, and Torch CPU. \n",
    "### By iterating multiple times, the performance comparison becomes more reliable.\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "num_iterations = 200\n",
    "\n",
    "# TensorFlow GPU\n",
    "tf_gpu_total_time = 0\n",
    "for _ in range(num_iterations):\n",
    "    with tf.device('/GPU:0'):\n",
    "        tf_gpu_start = tf.timestamp()\n",
    "        # Your TensorFlow GPU computations here\n",
    "        a = tf.random.normal((1000, 1000))\n",
    "        b = tf.random.normal((1000, 1000))\n",
    "        tf_gpu_result = tf.matmul(a, b)\n",
    "        tf_gpu_end = tf.timestamp()\n",
    "    tf_gpu_total_time += (tf_gpu_end - tf_gpu_start)\n",
    "\n",
    "# TensorFlow CPU\n",
    "tf_cpu_total_time = 0\n",
    "for _ in range(num_iterations):\n",
    "    with tf.device('/CPU:0'):\n",
    "        tf_cpu_start = tf.timestamp()\n",
    "        # Your TensorFlow CPU computations here\n",
    "        a = tf.random.normal((1000, 1000))\n",
    "        b = tf.random.normal((1000, 1000))\n",
    "        tf_cpu_result = tf.matmul(a, b)\n",
    "        tf_cpu_end = tf.timestamp()\n",
    "    tf_cpu_total_time += (tf_cpu_end - tf_cpu_start)\n",
    "\n",
    "# Torch GPU\n",
    "torch.cuda.init()\n",
    "torch_gpu_total_time = 0\n",
    "for _ in range(num_iterations):\n",
    "    torch_gpu_start = torch.cuda.Event(enable_timing=True)\n",
    "    torch_gpu_end = torch.cuda.Event(enable_timing=True)\n",
    "    torch_gpu_start.record()\n",
    "    # Your Torch GPU computations here\n",
    "    a = torch.randn(1000, 1000).cuda()\n",
    "    b = torch.randn(1000, 1000).cuda()\n",
    "    torch_gpu_result = torch.matmul(a, b)\n",
    "    torch_gpu_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    torch_gpu_total_time += torch_gpu_start.elapsed_time(torch_gpu_end) / 1000.0\n",
    "\n",
    "# Torch CPU\n",
    "torch_cpu_total_time = 0\n",
    "for _ in range(num_iterations):\n",
    "    torch_cpu_start = torch.cuda.Event(enable_timing=True)\n",
    "    torch_cpu_end = torch.cuda.Event(enable_timing=True)\n",
    "    torch_cpu_start.record()\n",
    "    # Your Torch CPU computations here\n",
    "    a = torch.randn(1000, 1000)\n",
    "    b = torch.randn(1000, 1000)\n",
    "    torch_cpu_result = torch.matmul(a, b)\n",
    "    torch_cpu_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    torch_cpu_total_time += torch_cpu_start.elapsed_time(torch_cpu_end) / 1000.0\n",
    "\n",
    "# Calculate average execution time\n",
    "tf_gpu_avg_execution_time = tf_gpu_total_time / num_iterations\n",
    "tf_cpu_avg_execution_time = tf_cpu_total_time / num_iterations\n",
    "torch_gpu_avg_execution_time = torch_gpu_total_time / num_iterations\n",
    "torch_cpu_avg_execution_time = torch_cpu_total_time / num_iterations\n",
    "\n",
    "# Compare performance\n",
    "print(f\"TensorFlow GPU average execution time: {tf_gpu_avg_execution_time} seconds\")\n",
    "print(f\"TensorFlow CPU average execution time: {tf_cpu_avg_execution_time} seconds\")\n",
    "print(f\"Torch GPU average execution time: {torch_gpu_avg_execution_time} seconds\")\n",
    "print(f\"Torch CPU average execution time: {torch_cpu_avg_execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Batch Size: 128\n",
      "Training on CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size: 128, Device: CPU:  63%|██████▎   | 1232/1950 [00:41<00:24, 29.08batch/s]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load CIFAR-10 dataset and get class names\n",
    "def load_cifar10():\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    return x_train, y_train, x_test, y_test, class_names  # Return class names\n",
    "\n",
    "# Function to create a CNN model\n",
    "def create_cnn_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, x_train, y_train, x_test, y_test, batch_size, epochs, device_name):\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tqdm(total=epochs * (len(x_train) // batch_size), desc=f\"Batch Size: {batch_size}, Device: {device_name}\", unit=\"batch\") as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                model.train_on_batch(x_batch, y_batch)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    return training_time\n",
    "\n",
    "# Function to evaluate the model and get accuracy and confusion matrix\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred_classes)\n",
    "    return accuracy, confusion_mat\n",
    "\n",
    "# Calculate specificity from the confusion matrix\n",
    "def calculate_specificity(confusion_matrix):\n",
    "    true_negatives = confusion_matrix.sum(axis=0) - confusion_matrix.diagonal()\n",
    "    false_positives = confusion_matrix.sum(axis=1) - confusion_matrix.diagonal()\n",
    "    specificity = true_negatives / (true_negatives + false_positives)\n",
    "    return specificity\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "def calculate_auc_roc(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred, average=\"macro\", multi_class=\"ovr\")\n",
    "    return auc_roc\n",
    "\n",
    "# Load CIFAR-10 dataset and get class names\n",
    "x_train, y_train, x_test, y_test, class_names = load_cifar10()\n",
    "\n",
    "# Set batch sizes to test\n",
    "batch_sizes = [128, 256, 512, 2048]\n",
    "epochs = 5  # Number of training epochs\n",
    "\n",
    "# Initialize results dictionary with \"Device\" column\n",
    "results = {\n",
    "    \"Device\": [],\n",
    "    \"Batch Size\": [],\n",
    "    \"Training Time (s)\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Recall\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"AUC-ROC\": [],\n",
    "    \"Confusion Matrix\": [],\n",
    "}\n",
    "\n",
    "# Train and evaluate the model with different batch sizes for both CPU and GPU\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Training with Batch Size: {batch_size}\")\n",
    "    \n",
    "    # Train on CPU\n",
    "    print(\"Training on CPU...\")\n",
    "    results[\"Device\"].append(\"CPU\")\n",
    "    with tf.device('/CPU:0'):\n",
    "        cpu_model = create_cnn_model()\n",
    "        cpu_training_time = train_model(cpu_model, x_train, y_train, x_test, y_test, batch_size, epochs, \"CPU\")\n",
    "    \n",
    "    cpu_accuracy, cpu_confusion = evaluate_model(cpu_model, x_test, y_test)\n",
    "    cpu_recall = recall_score(y_test, np.argmax(cpu_model.predict(x_test), axis=1), average=\"macro\")\n",
    "    cpu_precision = precision_score(y_test, np.argmax(cpu_model.predict(x_test), axis=1), average=\"macro\")\n",
    "    cpu_specificity = calculate_specificity(cpu_confusion)\n",
    "    cpu_auc_roc = calculate_auc_roc(cpu_model, x_test, y_test)\n",
    "\n",
    "    results[\"Batch Size\"].append(batch_size)\n",
    "    results[\"Training Time (s)\"].append(cpu_training_time)\n",
    "    results[\"Accuracy\"].append(cpu_accuracy)\n",
    "    results[\"Recall\"].append(cpu_recall)\n",
    "    results[\"Precision\"].append(cpu_precision)\n",
    "    results[\"Specificity\"].append(cpu_specificity.mean())\n",
    "    results[\"AUC-ROC\"].append(cpu_auc_roc)\n",
    "    results[\"Confusion Matrix\"].append(cpu_confusion)\n",
    "\n",
    "    # Train on GPU\n",
    "    print(\"Training on GPU...\")\n",
    "    results[\"Device\"].append(\"GPU\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        gpu_model = create_cnn_model()\n",
    "        gpu_training_time = train_model(gpu_model, x_train, y_train, x_test, y_test, batch_size, epochs, \"GPU\")\n",
    "    \n",
    "    gpu_accuracy, gpu_confusion = evaluate_model(gpu_model, x_test, y_test)\n",
    "    gpu_recall = recall_score(y_test, np.argmax(gpu_model.predict(x_test), axis=1), average=\"macro\")\n",
    "    gpu_precision = precision_score(y_test, np.argmax(gpu_model.predict(x_test), axis=1), average=\"macro\")\n",
    "    gpu_specificity = calculate_specificity(gpu_confusion)\n",
    "    gpu_auc_roc = calculate_auc_roc(gpu_model, x_test, y_test)\n",
    "\n",
    "    results[\"Batch Size\"].append(batch_size)\n",
    "    results[\"Training Time (s)\"].append(gpu_training_time)\n",
    "    results[\"Accuracy\"].append(gpu_accuracy)  # Corrected this line\n",
    "    results[\"Recall\"].append(gpu_recall)\n",
    "    results[\"Precision\"].append(gpu_precision)\n",
    "    results[\"Specificity\"].append(gpu_specificity.mean())\n",
    "    results[\"AUC-ROC\"].append(gpu_auc_roc)\n",
    "    results[\"Confusion Matrix\"].append(gpu_confusion)\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e312b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column combining \"Device\" and \"Batch Size\"\n",
    "df['Device_Batch'] = df['Device'] + \" (Batch Size \" + df['Batch Size'].astype(str) + \")\"\n",
    "\n",
    "# Plotting (Combined Metrics: Accuracy, Recall, Precision, Specificity, AUC-ROC)\n",
    "metrics = [\"Accuracy\", \"Recall\", \"Precision\", \"Specificity\", \"AUC-ROC\"]\n",
    "\n",
    "combined_metrics_data = []\n",
    "for metric in metrics:\n",
    "    for device_batch in df['Device_Batch'].unique():\n",
    "        metric_value = df[df['Device_Batch'] == device_batch][metric].values[0]\n",
    "        combined_metrics_data.append((metric, device_batch, metric_value))\n",
    "\n",
    "combined_metrics_df = pd.DataFrame(combined_metrics_data, columns=[\"Metric\", \"Device_Batch\", \"Value\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Metric\", y=\"Value\", hue=\"Device_Batch\", errorbar=None, data=combined_metrics_df)\n",
    "plt.title(\"Combined Metrics Comparison\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Device and Batch Size\", loc='lower right')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa255856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d9ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce48195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Time per Batch (Line Graph for GPU/CPU)\n",
    "training_time_data = df[df[\"Device\"].isin([\"CPU\", \"GPU\"])]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=\"Batch Size\", y=\"Training Time (s)\", hue=\"Device\", data=training_time_data, marker=\"o\")\n",
    "plt.title(\"Training Time per Batch (GPU vs. CPU)\")\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Training Time (s)\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Device\", loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46453bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store confusion matrix scores\n",
    "confusion_data = {\n",
    "    \"Device\": [],\n",
    "    \"Batch Size\": [],\n",
    "    \"Class\": [],\n",
    "    \"Class-specific Accuracy\": [],\n",
    "}\n",
    "\n",
    "# Iterate through devices and batch sizes to calculate scores by class\n",
    "for device in [\"CPU\", \"GPU\"]:\n",
    "    for batch_size in batch_sizes:\n",
    "        df_batch = df[(df[\"Device\"] == device) & (df[\"Batch Size\"] == batch_size)]\n",
    "        confusion_matrices = df_batch[\"Confusion Matrix\"].values\n",
    "\n",
    "        # Average the confusion matrices if there are multiple entries\n",
    "        avg_confusion_matrix = sum(confusion_matrices) / len(confusion_matrices)\n",
    "\n",
    "        # Calculate scores (0-1) for each class based on the confusion matrix\n",
    "        total_per_class = avg_confusion_matrix.sum(axis=1)\n",
    "        correct_per_class = np.diag(avg_confusion_matrix)\n",
    "        scores_per_class = correct_per_class / total_per_class\n",
    "\n",
    "        # Add the data to the dictionary\n",
    "        for class_index, class_name in enumerate(class_names):\n",
    "            confusion_data[\"Device\"].append(device)\n",
    "            confusion_data[\"Batch Size\"].append(batch_size)\n",
    "            confusion_data[\"Class\"].append(class_name)\n",
    "            confusion_data[\"Class-specific Accuracy\"].append(scores_per_class[class_index])\n",
    "\n",
    "# Create a DataFrame from the confusion data\n",
    "confusion_df = pd.DataFrame(confusion_data)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Class\", y=\"Class-specific Accuracy\", hue=\"Device\", data=confusion_df)\n",
    "plt.title(\"Confusion Matrix Scores by Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Class-specific Accuracy (0-1)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Device\", loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have a DataFrame named 'confusion_df' containing class-specific accuracy data\n",
    "# 'confusion_df' should have columns: 'Device', 'Batch Size', 'Class', and 'Class-specific Accuracy'\n",
    "\n",
    "# Pivot the data to create a heatmap grid\n",
    "heatmap_data = confusion_df.pivot(index='Class', columns=['Device', 'Batch Size'], values='Class-specific Accuracy')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt=\".2f\", cbar=True)\n",
    "plt.title('Class-Specific Accuracy Heatmap')\n",
    "plt.xlabel('Device, Batch Size')\n",
    "plt.ylabel('Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d66e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each class, GPU/CPU, and batch size combination\n",
    "summary_data = confusion_df.groupby(['Class', 'Device', 'Batch Size'])['Class-specific Accuracy'].agg(['mean', 'median']).reset_index()\n",
    "\n",
    "# Calculate the class-specific rank based on the 'mean' score for each combination\n",
    "summary_data['Class Specific Rank'] = summary_data.groupby(['Class'])['mean'].rank(ascending=False).astype(int)\n",
    "\n",
    "# Display the statistical summary\n",
    "print(summary_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= summary_data\n",
    "\n",
    "# Create a pivot table to count the occurrences of each rank for each combination\n",
    "pivot_table = pd.pivot_table(df, index=[\"Device\", \"Batch Size\"], columns=\"Class Specific Rank\", aggfunc='size', fill_value=0)\n",
    "\n",
    "# Calculate the weighted average for each combination\n",
    "weights = np.arange(1, len(pivot_table.columns) + 1)  # Weights for all ranks\n",
    "pivot_table[\"Weighted Average\"] = (pivot_table * weights).sum(axis=1) / pivot_table.sum(axis=1)\n",
    "\n",
    "# Reset the index to make \"Device\" and \"Batch Size\" columns\n",
    "pivot_table.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns for better clarity\n",
    "pivot_table.columns.name = None\n",
    "\n",
    "# Add an \"Overall Rank\" column based on \"Weighted Average\" and sort by it in ascending order\n",
    "pivot_table[\"Overall Rank\"] = pivot_table[\"Weighted Average\"].rank()\n",
    "\n",
    "# Sort the DataFrame by \"Weighted Average\" in ascending order\n",
    "pivot_table.sort_values(by=\"Weighted Average\", ascending=True, inplace=True)\n",
    "\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85487f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037b4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
